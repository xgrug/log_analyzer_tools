#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Nginx è¯·æ±‚é¢‘ç‡è‡ªåŠ¨åŒ–åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
"""

import re
import gzip
import json
import math
import statistics
import html
import argparse
import os
import sys
from collections import defaultdict, Counter, deque
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Set, Any, Optional, Iterator
from dataclasses import dataclass, asdict
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# ==================== é…ç½®å¸¸é‡ ====================

LOG_PATTERN = re.compile(
    r'(?P<ip>\S+) \S+ \S+ \[(?P<time>[^\]]+)\] '
    r'"(?P<method>\S+) (?P<path>\S+) \S+" '
    r'(?P<status>\d+)'
)

TIME_FORMAT = "%d/%b/%Y:%H:%M:%S %z"

DEFAULT_SENSITIVE_PATHS = {
    '/login', '/register', '/reset', '/forgot', '/password',
    '/admin', '/manage', '/console', '/debug', '/dashboard',
    '/api/admin', '/api/auth', '/export', '/backup', '/upload',
    '/.env', '/.git', '/config', '/phpMyAdmin', '/phpmyadmin',
    '/wp-login.php', '/wp-admin', '/actuator', '/swagger'
}

TIME_WINDOWS = {
    '10m': 600,
    '30m': 1800,
    '1h': 3600,
    '24h': 86400
}

# ä¼˜åŒ–: ä½¿ç”¨æœ‰é™å¤§å°çš„å†…å­˜ç¼“å­˜
MAX_CACHE_SIZE = 10000


# ==================== æ•°æ®ç±» ====================

@dataclass
class LogEntry:
    """å•æ¡æ—¥å¿—è®°å½•"""
    ip: str
    path: str
    status: str
    timestamp: datetime
    method: str = ""


@dataclass
class RateStats:
    """é¢‘ç‡ç»Ÿè®¡æ•°æ®"""
    avg_qps: float
    max_qps: float
    p95_qps: float
    p99_qps: float
    total_requests: int
    suggest_rps: int
    human_readable: str
    volatility: float = 0.0


@dataclass
class PathAnalysis:
    """è·¯å¾„åˆ†æç»“æœ"""
    path: str
    stats: RateStats
    is_sensitive: bool
    window_suggestions: Dict[str, int]
    window_human: Dict[str, str]
    unique_ips: int = 0


@dataclass
class IPAnalysis:
    """IPåˆ†æç»“æœ"""
    ip: str
    stats: RateStats
    unique_paths: int = 0
    error_rate: float = 0.0


@dataclass
class RiskItem:
    """é£é™©é¡¹"""
    ip: str
    path: str
    risk_score: float
    stats: RateStats
    reason: str = ""


# ==================== å·¥å…·å‡½æ•° ====================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self, total: int, desc: str = "Processing"):
        self.total = total
        self.current = 0
        self.desc = desc
        self.last_percent = -1

    def update(self, amount: int = 1):
        self.current += amount
        if self.total > 0:
            percent = int(100 * self.current / self.total)
            if percent != self.last_percent and percent % 5 == 0:
                print(f"[{self.desc}] {percent}% ({self.current:,}/{self.total:,})",
                      file=sys.stderr, flush=True)
                self.last_percent = percent


class TimeWindowCache:
    """æ—¶é—´çª—å£èšåˆç¼“å­˜"""

    def __init__(self, max_size: int = MAX_CACHE_SIZE):
        self.cache: Dict[Tuple[str, int], List[float]] = {}
        self.max_size = max_size

    def get(self, key: str, window: int, per_sec: Counter) -> List[float]:
        cache_key = (key, window)
        if cache_key in self.cache:
            return self.cache[cache_key]

        result = aggregate_requests_by_window(per_sec, window)

        if len(self.cache) < self.max_size:
            self.cache[cache_key] = result

        return result


def parse_log_time(time_str: str) -> Optional[datetime]:
    """è§£ææ—¥å¿—æ—¶é—´ï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
    try:
        return datetime.strptime(time_str, TIME_FORMAT)
    except ValueError:
        # å¤„ç†æ—¶åŒºæ ¼å¼é—®é¢˜
        if len(time_str) >= 5 and time_str[-5] not in ('+', '-'):
            try:
                fixed = time_str[:-5] + ' ' + time_str[-5:]
                return datetime.strptime(fixed, TIME_FORMAT)
            except ValueError:
                pass
    return None


def open_log_file(filepath: str):
    """æ™ºèƒ½æ‰“å¼€æ—¥å¿—æ–‡ä»¶"""
    if filepath.endswith('.gz'):
        return gzip.open(filepath, 'rt', encoding='utf-8', errors='ignore')
    return open(filepath, 'r', encoding='utf-8', errors='ignore')


def compute_percentile(data: List[float], p: float) -> float:
    """è®¡ç®—ç™¾åˆ†ä½æ•°"""
    if not data:
        return 0.0
    sorted_data = sorted(data)
    index = int(len(sorted_data) * p / 100)
    return sorted_data[min(index, len(sorted_data) - 1)]


def calculate_adaptive_threshold(
        data: List[float],
        method: str = "percentile",
        sensitivity: float = 1.0
) -> float:
    """è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—"""
    if not data:
        return 1.0

    if method == "percentile":
        return max(1.0, compute_percentile(data, 95) * sensitivity)

    elif method == "mean_std":
        if len(data) < 2:
            return float(data[0]) if data else 1.0
        mean_val = statistics.mean(data)
        std_dev = statistics.stdev(data) if len(data) > 1 else 0
        return max(1.0, (mean_val + 2 * std_dev) * sensitivity)

    elif method == "iqr":
        if len(data) < 4:
            return max(1.0, max(data) * sensitivity) if data else 1.0
        q1 = compute_percentile(data, 25)
        q3 = compute_percentile(data, 75)
        iqr = q3 - q1
        upper_fence = q3 + 1.5 * iqr
        return max(1.0, upper_fence * sensitivity)

    elif method == "mad":  # æ–°å¢: Median Absolute Deviation
        if len(data) < 2:
            return float(data[0]) if data else 1.0
        median = statistics.median(data)
        mad = statistics.median([abs(x - median) for x in data])
        return max(1.0, (median + 3 * mad) * sensitivity)

    return max(1.0, compute_percentile(data, 95) * sensitivity)


def aggregate_requests_by_window(
        per_sec_counter: Counter,
        window_seconds: int
) -> List[float]:
    """å°†è¯·æ±‚èšåˆåˆ°æ—¶é—´çª—å£"""
    if not per_sec_counter:
        return []

    time_points = sorted(
        (datetime.strptime(k, "%Y-%m-%d %H:%M:%S"), v)
        for k, v in per_sec_counter.items()
    )

    if not time_points:
        return []

    start_time = time_points[0][0]
    end_time = time_points[-1][0]
    current = start_time
    window_qps = []
    i = 0

    while current <= end_time:
        window_end = current + timedelta(seconds=window_seconds)
        total_in_window = 0

        while i < len(time_points) and time_points[i][0] < window_end:
            total_in_window += time_points[i][1]
            i += 1

        avg_qps = total_in_window / window_seconds if window_seconds > 0 else 0
        window_qps.append(avg_qps)
        current = window_end

    return window_qps


def rps_to_human_readable(rate_rps: float, max_denom: int = 3600) -> str:
    """å°† r/s è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼"""
    if rate_rps <= 0:
        return "ç¦æ­¢è®¿é—®"

    candidates = [
        (10, "10ç§’"),
        (30, "30ç§’"),
        (60, "åˆ†é’Ÿ"),
        (300, "5åˆ†é’Ÿ"),
        (600, "10åˆ†é’Ÿ"),
        (1800, "30åˆ†é’Ÿ"),
        (3600, "å°æ—¶")
    ]

    best_desc = f"{rate_rps:.1f} æ¬¡/ç§’"
    min_error = float('inf')

    for seconds, name in candidates:
        if seconds > max_denom:
            continue
        total_requests = rate_rps * seconds
        rounded = math.ceil(total_requests)
        actual_rps = rounded / seconds
        error = abs(actual_rps - rate_rps)

        if error < min_error or (error == min_error and rounded == int(rounded)):
            min_error = error
            if name in ("åˆ†é’Ÿ", "å°æ—¶"):
                best_desc = f"æ¯{name}æœ€å¤š{rounded}æ¬¡"
            else:
                best_desc = f"æ¯{name}æœ€å¤š{rounded}æ¬¡"

    return best_desc


def detect_anomalies(qps_list: List[float], threshold: float = 3.0) -> List[int]:
    """æ£€æµ‹å¼‚å¸¸å€¼ç´¢å¼•"""
    if len(qps_list) < 10:
        return []

    median = statistics.median(qps_list)
    mad = statistics.median([abs(x - median) for x in qps_list])

    if mad == 0:
        return []

    anomalies = []
    for i, qps in enumerate(qps_list):
        z_score = abs(qps - median) / (mad * 1.4826)  # 1.4826 æ˜¯å¸¸æ•°è½¬æ¢å› å­
        if z_score > threshold:
            anomalies.append(i)

    return anomalies


# ==================== æ—¥å¿—è§£æ ====================

class LogParser:
    """ä¼˜åŒ–çš„æ—¥å¿—è§£æå™¨"""

    def __init__(self,
                 time_window: Optional[Tuple[datetime, datetime]] = None,
                 target_ip: Optional[str] = None,
                 target_path: Optional[str] = None):
        self.time_window = time_window
        self.target_ip = target_ip
        self.target_path = target_path
        self.stats = {
            'total_lines': 0,
            'parsed_lines': 0,
            'filtered_lines': 0,
            'errors': 0
        }

    def parse_line(self, line: str) -> Optional[LogEntry]:
        """è§£æå•è¡Œæ—¥å¿—"""
        self.stats['total_lines'] += 1

        match = LOG_PATTERN.search(line)
        if not match:
            return None

        log_time = parse_log_time(match.group('time'))
        if not log_time:
            self.stats['errors'] += 1
            return None

        # æ—¶é—´çª—å£è¿‡æ»¤
        if self.time_window:
            start, end = self.time_window
            if log_time < start or log_time > end:
                return None

        ip = match.group('ip')
        raw_path = match.group('path')
        clean_path = raw_path.split('?')[0].rstrip('/')
        status = match.group('status')
        method = match.group('method')

        # IP/è·¯å¾„è¿‡æ»¤
        if self.target_ip and ip != self.target_ip:
            return None
        if self.target_path and clean_path != self.target_path:
            return None

        self.stats['parsed_lines'] += 1

        return LogEntry(
            ip=ip,
            path=clean_path,
            status=status,
            timestamp=log_time,
            method=method
        )

    def parse_file(self, filepath: str) -> Iterator[LogEntry]:
        """æµå¼è§£ææ–‡ä»¶"""
        try:
            with open_log_file(filepath) as f:
                for line in f:
                    entry = self.parse_line(line)
                    if entry:
                        yield entry
        except Exception as e:
            print(f"[ERROR] è¯»å– {filepath} å¤±è´¥: {e}", file=sys.stderr)
            self.stats['errors'] += 1


class DataAggregator:
    """æ•°æ®èšåˆå™¨"""

    def __init__(self, sensitive_paths: Set[str]):
        self.sensitive_paths = sensitive_paths
        self.global_per_sec = Counter()
        self.path_per_sec = defaultdict(Counter)
        self.ip_per_sec = defaultdict(Counter)
        self.ip_path_per_sec = defaultdict(Counter)
        self.status_code_stats = Counter()
        self.ip_paths = defaultdict(set)
        self.path_ips = defaultdict(set)
        self.ip_errors = defaultdict(int)
        self.actual_start: Optional[datetime] = None
        self.actual_end: Optional[datetime] = None

    def add_entry(self, entry: LogEntry):
        """æ·»åŠ æ—¥å¿—æ¡ç›®"""
        sec_key = entry.timestamp.strftime("%Y-%m-%d %H:%M:%S")

        # æ›´æ–°è®¡æ•°å™¨
        self.global_per_sec[sec_key] += 1
        self.path_per_sec[entry.path][sec_key] += 1
        self.ip_per_sec[entry.ip][sec_key] += 1
        self.status_code_stats[entry.status] += 1

        # è®°å½• IP-è·¯å¾„å…³ç³»
        self.ip_paths[entry.ip].add(entry.path)
        self.path_ips[entry.path].add(entry.ip)

        # é”™è¯¯ç»Ÿè®¡
        if entry.status.startswith('4') or entry.status.startswith('5'):
            self.ip_errors[entry.ip] += 1

        # æ•æ„Ÿè·¯å¾„ç‰¹æ®Šå¤„ç†
        if entry.path in self.sensitive_paths:
            self.ip_path_per_sec[(entry.ip, entry.path)][sec_key] += 1

        # æ›´æ–°æ—¶é—´èŒƒå›´
        if self.actual_start is None:
            self.actual_start = self.actual_end = entry.timestamp
        else:
            if entry.timestamp < self.actual_start:
                self.actual_start = entry.timestamp
            if entry.timestamp > self.actual_end:
                self.actual_end = entry.timestamp

    def get_result(self) -> Dict[str, Any]:
        """è·å–èšåˆç»“æœ"""
        if self.actual_start is None:
            now = datetime.now()
            self.actual_start = self.actual_end = now

        return {
            'global_per_sec': self.global_per_sec,
            'path_per_sec': self.path_per_sec,
            'ip_per_sec': self.ip_per_sec,
            'ip_path_per_sec': self.ip_path_per_sec,
            'status_code_stats': self.status_code_stats,
            'ip_paths': self.ip_paths,
            'path_ips': self.path_ips,
            'ip_errors': self.ip_errors,
            'time_window': (self.actual_start, self.actual_end)
        }


def analyze_logs_optimized(
        logfiles: List[str],
        time_window: Optional[Tuple[datetime, datetime]],
        sensitive_paths: Set[str],
        target_ip: Optional[str] = None,
        target_path: Optional[str] = None,
        max_workers: int = 4
) -> Dict[str, Any]:
    """ä¼˜åŒ–çš„æ—¥å¿—åˆ†æï¼ˆæ”¯æŒå¹¶å‘ï¼‰"""

    if time_window:
        start, end = time_window
        print(f"[INFO] åˆ†ææ—¶é—´çª—å£: {start} â†’ {end}", file=sys.stderr)
    else:
        print("[INFO] åˆ†æå…¨éƒ¨æ—¥å¿—æ•°æ®", file=sys.stderr)

    parser = LogParser(time_window, target_ip, target_path)
    aggregator = DataAggregator(sensitive_paths)

    # å•æ–‡ä»¶æƒ…å†µä¸ä½¿ç”¨å¹¶å‘
    if len(logfiles) == 1:
        for entry in parser.parse_file(logfiles[0]):
            aggregator.add_entry(entry)
    else:
        # å¤šæ–‡ä»¶å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(list, parser.parse_file(f)): f
                for f in logfiles
            }

            for future in as_completed(futures):
                filepath = futures[future]
                try:
                    entries = future.result()
                    for entry in entries:
                        aggregator.add_entry(entry)
                    print(f"[INFO] å®Œæˆ: {filepath}", file=sys.stderr)
                except Exception as e:
                    print(f"[ERROR] {filepath}: {e}", file=sys.stderr)

    result = aggregator.get_result()
    result['stats'] = parser.stats
    result['total_processed'] = parser.stats['parsed_lines']

    print(f"[INFO] å¤„ç†å®Œæˆ: {result['total_processed']:,} æ¡æœ‰æ•ˆæ—¥å¿—", file=sys.stderr)
    return result


# ==================== ç»Ÿè®¡åˆ†æ ====================

def create_rate_stats(qps_list: List[float], sensitivity: float = 0.85) -> RateStats:
    """åˆ›å»ºé¢‘ç‡ç»Ÿè®¡å¯¹è±¡"""
    if not qps_list:
        return RateStats(0, 0, 0, 0, 0, 1, "æ¯10ç§’æœ€å¤š1æ¬¡")

    total = sum(qps_list)
    avg = statistics.mean(qps_list)
    max_qps = max(qps_list)
    p95 = compute_percentile(qps_list, 95)
    p99 = compute_percentile(qps_list, 99)

    # ä½¿ç”¨å¤šç§æ–¹æ³•è®¡ç®—é˜ˆå€¼
    thresholds = [
        calculate_adaptive_threshold(qps_list, method, sensitivity)
        for method in ["percentile", "mean_std", "iqr", "mad"]
    ]
    suggest_rps = max(1, round(statistics.median(thresholds)))

    # è®¡ç®—æ³¢åŠ¨æ€§
    volatility = (statistics.stdev(qps_list) / avg) if len(qps_list) > 1 and avg > 0 else 0

    return RateStats(
        avg_qps=avg,
        max_qps=max_qps,
        p95_qps=p95,
        p99_qps=p99,
        total_requests=int(total),
        suggest_rps=suggest_rps,
        human_readable=rps_to_human_readable(suggest_rps),
        volatility=volatility
    )


def analyze_global_stats(global_per_sec: Counter, cache: TimeWindowCache) -> Dict[str, Any]:
    """åˆ†æå…¨å±€ç»Ÿè®¡"""
    base_qps = list(global_per_sec.values())

    if not base_qps:
        fallback_stats = RateStats(0, 0, 0, 0, 0, 10, "æ¯10ç§’æœ€å¤š10æ¬¡")
        return {
            'base': fallback_stats,
            'windows': {name: fallback_stats for name in TIME_WINDOWS}
        }

    base_stats = create_rate_stats(base_qps, sensitivity=0.8)

    window_stats = {}
    for name, seconds in TIME_WINDOWS.items():
        window_qps = cache.get('global', seconds, global_per_sec)
        if window_qps:
            stats = create_rate_stats(window_qps, sensitivity=0.8)
            stats.human_readable = rps_to_human_readable(stats.suggest_rps, max_denom=seconds)
            window_stats[name] = stats
        else:
            window_stats[name] = base_stats

    return {'base': base_stats, 'windows': window_stats}


def analyze_paths(
        path_per_sec: Dict[str, Counter],
        sensitive_paths: Set[str],
        path_ips: Dict[str, Set[str]],
        cache: TimeWindowCache
) -> List[PathAnalysis]:
    """åˆ†æè·¯å¾„ç»Ÿè®¡"""
    results = []

    for path, sec_counter in path_per_sec.items():
        qps_list = list(sec_counter.values())
        if not qps_list:
            continue

        base_stats = create_rate_stats(qps_list, sensitivity=0.85)

        # è®¡ç®—å„æ—¶é—´çª—å£å»ºè®®
        window_suggestions = {}
        window_human = {}

        for name, seconds in TIME_WINDOWS.items():
            window_qps = cache.get(f'path_{path}', seconds, sec_counter)
            if window_qps:
                stats = create_rate_stats(window_qps, sensitivity=0.85)
                window_suggestions[name] = stats.suggest_rps
                window_human[name] = rps_to_human_readable(stats.suggest_rps, max_denom=seconds)
            else:
                window_suggestions[name] = base_stats.suggest_rps
                window_human[name] = base_stats.human_readable

        results.append(PathAnalysis(
            path=path,
            stats=base_stats,
            is_sensitive=path in sensitive_paths,
            window_suggestions=window_suggestions,
            window_human=window_human,
            unique_ips=len(path_ips.get(path, set()))
        ))

    return sorted(results, key=lambda x: x.stats.total_requests, reverse=True)


def analyze_ips(
        ip_per_sec: Dict[str, Counter],
        ip_paths: Dict[str, Set[str]],
        ip_errors: Dict[str, int]
) -> List[IPAnalysis]:
    """åˆ†æIPç»Ÿè®¡"""
    results = []

    for ip, sec_counter in ip_per_sec.items():
        qps_list = list(sec_counter.values())
        if not qps_list:
            continue

        stats = create_rate_stats(qps_list, sensitivity=0.8)
        total_req = stats.total_requests
        error_count = ip_errors.get(ip, 0)
        error_rate = (error_count / total_req * 100) if total_req > 0 else 0

        results.append(IPAnalysis(
            ip=ip,
            stats=stats,
            unique_paths=len(ip_paths.get(ip, set())),
            error_rate=error_rate
        ))

    return sorted(results, key=lambda x: x.stats.total_requests, reverse=True)


def analyze_risks(
        ip_path_per_sec: Dict[Tuple[str, str], Counter],
        ip_errors: Dict[str, int]
) -> List[RiskItem]:
    """åˆ†æé£é™©é¡¹"""
    results = []

    for (ip, path), sec_counter in ip_path_per_sec.items():
        qps_list = list(sec_counter.values())
        if not qps_list:
            continue

        stats = create_rate_stats(qps_list, sensitivity=0.7)

        # å¢å¼ºçš„é£é™©è¯„åˆ†
        base_score = (stats.max_qps * 0.5 + stats.p95_qps * 0.3 + stats.p99_qps * 0.2)
        volume_factor = math.log(stats.total_requests + 1)
        error_count = ip_errors.get(ip, 0)
        error_factor = 1 + (error_count / stats.total_requests if stats.total_requests > 0 else 0)

        risk_score = base_score * volume_factor * error_factor

        # é£é™©é˜ˆå€¼
        if stats.max_qps >= 10 or stats.total_requests > 200 or risk_score > 50:
            reason = []
            if stats.max_qps >= 10:
                reason.append(f"é«˜å³°å€¼QPS({stats.max_qps})")
            if stats.total_requests > 500:
                reason.append(f"å¤§é‡è¯·æ±‚({stats.total_requests})")
            if error_count > 50:
                reason.append(f"é«˜é”™è¯¯ç‡({error_count})")

            results.append(RiskItem(
                ip=ip,
                path=path,
                risk_score=round(risk_score, 2),
                stats=stats,
                reason=" | ".join(reason) if reason else "ç»¼åˆé£é™©"
            ))

    return sorted(results, key=lambda x: x.risk_score, reverse=True)


# ==================== HTML ç”Ÿæˆ ====================

def generate_html_report(
        start: datetime,
        end: datetime,
        total: int,
        global_result: Dict[str, Any],
        path_analyses: List[PathAnalysis],
        ip_analyses: List[IPAnalysis],
        risk_items: List[RiskItem],
        status_stats: List[Dict[str, Any]],
        nginx_conf: str,
        target_ip: Optional[str] = None,
        target_path: Optional[str] = None
) -> str:
    """ç”ŸæˆHTMLæŠ¥å‘Š"""

    # æ ‡é¢˜å’Œè¿‡æ»¤ä¿¡æ¯
    title_parts = ["Nginx è¯·æ±‚é¢‘ç‡åˆ†ææŠ¥å‘Š"]
    filter_info_html = ""

    if target_ip or target_path:
        filters = []
        if target_ip:
            filters.append(f"IP={target_ip}")
            title_parts.append(f"(IP: {target_ip})")
        if target_path:
            filters.append(f"è·¯å¾„={target_path}")
            title_parts.append(f"(è·¯å¾„: {target_path})")
        filter_info_html = f'<div class="filter-info"><p><strong>ç­›é€‰æ¡ä»¶:</strong> {" & ".join(filters)}</p></div>'

    title = " ".join(title_parts)

    # å…¨å±€ç»Ÿè®¡
    base_stats = global_result['base']
    global_html = f"""
        <h2>ğŸŒ å…¨å±€è¯·æ±‚é¢‘ç‡ (QPS)</h2>
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-value">{base_stats.avg_qps:.1f}</div>
                <div class="metric-label">å¹³å‡ QPS</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{base_stats.max_qps:.0f}</div>
                <div class="metric-label">å³°å€¼ QPS</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{base_stats.p95_qps:.1f}</div>
                <div class="metric-label">P95 QPS</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{base_stats.volatility:.2f}</div>
                <div class="metric-label">æ³¢åŠ¨æ€§</div>
            </div>
        </div>
        <div class="recommendation">
            <p><strong>ğŸ’¡ å»ºè®®å…¨å±€é™æµ:</strong> <code>rate={base_stats.suggest_rps}r/s</code> 
            <span class="human-readable">({html.escape(base_stats.human_readable)})</span></p>
        </div>
        <h3>â±ï¸ å¤šæ—¶é—´çª—å£é™æµå»ºè®®</h3>
        <table class="stats-table">
            <tr><th>çª—å£</th><th>å»ºè®® (r/s)</th><th>å¯è¯»å»ºè®®</th><th>P95 QPS</th></tr>
    """

    for name in ['10m', '30m', '1h', '24h']:
        w = global_result['windows'][name]
        global_html += f"<tr><td><strong>{name}</strong></td><td>{w.suggest_rps}</td><td>{html.escape(w.human_readable)}</td><td>{w.p95_qps:.1f}</td></tr>"
    global_html += "</table>"

    # è·¯å¾„ç»Ÿè®¡
    sensitive = [p for p in path_analyses if p.is_sensitive]
    normal = [p for p in path_analyses if not p.is_sensitive]

    paths_html = '<h2>ğŸš€ è·¯å¾„è¯·æ±‚é¢‘ç‡åˆ†æ</h2>'

    if sensitive:
        paths_html += '<h3>âš ï¸ æ•æ„Ÿè·¯å¾„</h3><table class="stats-table sensitive">'
        paths_html += '<tr><th>è·¯å¾„</th><th>æ€»è°ƒç”¨</th><th>ç‹¬ç«‹IP</th><th>å¹³å‡QPS</th><th>P95</th><th>æ³¢åŠ¨æ€§</th><th>å»ºè®®é™æµ</th></tr>'
        for p in sensitive[:50]:
            paths_html += f"""<tr>
                <td><code>{html.escape(p.path)}</code></td>
                <td>{p.stats.total_requests:,}</td>
                <td>{p.unique_ips}</td>
                <td>{p.stats.avg_qps:.2f}</td>
                <td>{p.stats.p95_qps:.1f}</td>
                <td>{p.stats.volatility:.2f}</td>
                <td>{p.stats.suggest_rps} r/s<br><small>{html.escape(p.stats.human_readable)}</small></td>
            </tr>"""
        paths_html += '</table>'

    if normal:
        paths_html += '<h3>ğŸ“Š æ™®é€šè·¯å¾„ (Top 100)</h3><table class="stats-table">'
        paths_html += '<tr><th>è·¯å¾„</th><th>æ€»è°ƒç”¨</th><th>ç‹¬ç«‹IP</th><th>å¹³å‡QPS</th><th>P95</th><th>å»ºè®®é™æµ</th></tr>'
        for p in normal[:100]:
            vol_class = "high-volatility" if p.stats.volatility > 1.0 else ""
            paths_html += f"""<tr class="{vol_class}">
                <td><code>{html.escape(p.path)}</code></td>
                <td>{p.stats.total_requests:,}</td>
                <td>{p.unique_ips}</td>
                <td>{p.stats.avg_qps:.2f}</td>
                <td>{p.stats.p95_qps:.1f}</td>
                <td>{p.stats.suggest_rps} r/s</td>
            </tr>"""
        if len(normal) > 100:
            paths_html += f'<tr><td colspan="6" class="more-info">è¿˜æœ‰ {len(normal) - 100} æ¡æœªæ˜¾ç¤º</td></tr>'
        paths_html += '</table>'

    # IPç»Ÿè®¡
    ips_html = '<h2>ğŸ–¥ï¸ IP è¯·æ±‚é¢‘ç‡åˆ†æ (Top 100)</h2><table class="stats-table">'
    ips_html += '<tr><th>IPåœ°å€</th><th>æ€»è¯·æ±‚</th><th>ç‹¬ç«‹è·¯å¾„</th><th>å¹³å‡QPS</th><th>P95 QPS</th><th>é”™è¯¯ç‡</th><th>å»ºè®®é™æµ</th></tr>'
    for ip_analysis in ip_analyses[:100]:
        error_class = "high-error" if ip_analysis.error_rate > 10 else ""
        ips_html += f"""<tr class="{error_class}">
            <td><code>{html.escape(ip_analysis.ip)}</code></td>
            <td>{ip_analysis.stats.total_requests:,}</td>
            <td>{ip_analysis.unique_paths}</td>
            <td>{ip_analysis.stats.avg_qps:.2f}</td>
            <td>{ip_analysis.stats.p95_qps:.1f}</td>
            <td>{ip_analysis.error_rate:.1f}%</td>
            <td>{ip_analysis.stats.suggest_rps} r/s</td>
        </tr>"""
    if len(ip_analyses) > 100:
        ips_html += f'<tr><td colspan="7" class="more-info">è¿˜æœ‰ {len(ip_analyses) - 100} ä¸ªIPæœªæ˜¾ç¤º</td></tr>'
    ips_html += '</table>'

    # é£é™©é¡¹
    risks_html = '<h2>ğŸ›¡ï¸ é«˜é£é™© IP + è·¯å¾„ç»„åˆï¼ˆé˜²æš´åŠ›ç ´è§£ï¼‰</h2>'
    if not risk_items:
        risks_html += '<p class="success-msg">âœ… æœªå‘ç°é«˜é£é™©è¡Œä¸º</p>'
    else:
        risks_html += '<table class="stats-table risk-table">'
        risks_html += '<tr><th>IP</th><th>è·¯å¾„</th><th>æ€»è¯·æ±‚</th><th>å³°å€¼QPS</th><th>P95</th><th>é£é™©è¯„åˆ†</th><th>åŸå› </th><th>å»ºè®®é™æµ</th></tr>'
        for risk in risk_items[:30]:
            risk_level = "critical" if risk.risk_score > 200 else "high" if risk.risk_score > 100 else "medium"
            risks_html += f"""<tr class="risk-{risk_level}">
                <td><code>{html.escape(risk.ip)}</code></td>
                <td><code>{html.escape(risk.path)}</code></td>
                <td>{risk.stats.total_requests:,}</td>
                <td>{risk.stats.max_qps:.0f}</td>
                <td>{risk.stats.p95_qps:.1f}</td>
                <td><strong>{risk.risk_score}</strong></td>
                <td><small>{html.escape(risk.reason)}</small></td>
                <td>{risk.stats.suggest_rps} r/s</td>
            </tr>"""
        risks_html += '</table>'

    # çŠ¶æ€ç ç»Ÿè®¡
    status_html = '<h2>ğŸ“ˆ HTTP çŠ¶æ€ç åˆ†å¸ƒ</h2><table class="stats-table status-table">'
    status_html += '<tr><th>çŠ¶æ€ç </th><th>æ•°é‡</th><th>å æ¯”</th><th>ç±»å‹</th></tr>'
    for stat in status_stats:
        type_class = stat['type'].replace('_', '-')
        status_html += f"""<tr class="{type_class}">
            <td><strong>{stat['code']}</strong></td>
            <td>{stat['count']:,}</td>
            <td>{stat['percentage']:.2f}%</td>
            <td>{stat['type']}</td>
        </tr>"""
    status_html += '</table>'

    # å®Œæ•´HTML
    html_content = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{html.escape(title)}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
               background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; }}
        .container {{ max-width: 1400px; margin: 0 auto; background: white; padding: 40px; 
                     border-radius: 12px; box-shadow: 0 20px 60px rgba(0,0,0,0.3); }}
        h1 {{ color: #2c3e50; margin-bottom: 10px; font-size: 2.5em; }}
        h2 {{ color: #34495e; margin: 30px 0 20px; padding-bottom: 10px; 
             border-bottom: 3px solid #3498db; }}
        h3 {{ color: #555; margin: 20px 0 10px; }}

        .summary {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                   color: white; padding: 20px; border-radius: 8px; margin: 20px 0; }}
        .summary p {{ margin: 5px 0; font-size: 1.1em; }}

        .filter-info {{ background: #e3f2fd; padding: 15px; border-radius: 8px; 
                       margin: 15px 0; border-left: 4px solid #2196f3; }}

        .metrics-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                        gap: 15px; margin: 20px 0; }}
        .metric-card {{ background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
                       padding: 20px; border-radius: 8px; text-align: center; 
                       box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .metric-value {{ font-size: 2.5em; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ font-size: 0.9em; color: #7f8c8d; margin-top: 5px; }}

        .recommendation {{ background: #d4edda; border: 1px solid #c3e6cb; 
                         padding: 15px; border-radius: 8px; margin: 20px 0; }}
        .recommendation code {{ background: #fff; padding: 4px 8px; border-radius: 4px;
                               color: #d63384; font-weight: bold; }}
        .human-readable {{ color: #28a745; font-style: italic; }}

        .stats-table {{ width: 100%; border-collapse: collapse; margin: 15px 0; 
                       box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
        .stats-table th {{ background: #34495e; color: white; padding: 12px; text-align: left; }}
        .stats-table td {{ padding: 10px; border-bottom: 1px solid #ecf0f1; }}
        .stats-table tr:hover {{ background: #f8f9fa; }}
        .stats-table.sensitive {{ border-left: 4px solid #ff9800; }}
        .stats-table.risk-table {{ border-left: 4px solid #f44336; }}

        .high-volatility {{ background: #fff3cd !important; }}
        .high-error {{ background: #f8d7da !important; }}
        .risk-critical {{ background: #f8d7da !important; font-weight: bold; }}
        .risk-high {{ background: #fff3cd !important; }}
        .risk-medium {{ background: #d1ecf1 !important; }}

        .success {{ background: #d4edda !important; }}
        .client-error {{ background: #fff3cd !important; }}
        .server-error {{ background: #f8d7da !important; }}

        .more-info {{ text-align: center; color: #6c757d; font-style: italic; }}
        .success-msg {{ color: #28a745; font-size: 1.2em; padding: 20px; text-align: center; }}

        pre {{ background: #2d2d2d; color: #f8f8f2; padding: 20px; border-radius: 8px; 
              overflow-x: auto; line-height: 1.5; }}
        code {{ font-family: 'Courier New', monospace; }}

        @media print {{ body {{ background: white; }} .container {{ box-shadow: none; }} }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š {html.escape(title)}</h1>
        <div class="summary">
            <p><strong>â° æ—¶é—´çª—å£:</strong> {start.strftime('%Y-%m-%d %H:%M:%S')} â†’ {end.strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p><strong>ğŸ“ æ€»è¯·æ±‚æ•°:</strong> {total:,}</p>
            <p><strong>ğŸ” è·¯å¾„æ•°:</strong> {len(path_analyses)} | <strong>ğŸ–¥ï¸ ç‹¬ç«‹IP:</strong> {len(ip_analyses)} | <strong>âš ï¸ é£é™©é¡¹:</strong> {len(risk_items)}</p>
        </div>
        {filter_info_html}
        {global_html}
        {paths_html}
        {ips_html}
        {risks_html}
        {status_html}
        <h2>âš™ï¸ Nginx é™æµé…ç½®å»ºè®®</h2>
        <pre>{html.escape(nginx_conf)}</pre>
        <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; 
                      text-align: center; color: #6c757d; font-size: 0.9em;">
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Nginx Rate Analyzer v3.0</p>
        </footer>
    </div>
</body>
</html>"""

    return html_content


def generate_nginx_config(
        global_result: Dict[str, Any],
        path_analyses: List[PathAnalysis]
) -> str:
    """ç”ŸæˆNginxé…ç½®å»ºè®®"""
    base = global_result['base']

    config = f"""# ===== Nginx é™æµé…ç½®å»ºè®® =====
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

# 1. å…¨å±€é™æµåŒºåŸŸ
limit_req_zone $binary_remote_addr zone=global:10m rate={base.suggest_rps}r/s;

# 2. è·¯å¾„çº§é™æµåŒºåŸŸ
"""

    for p in path_analyses:
        if p.is_sensitive or p.stats.total_requests > 1000:
            zone_name = re.sub(r'[^a-zA-Z0-9]', '_', p.path.lstrip('/')) or "default"
            config += f"limit_req_zone $binary_remote_addr zone={zone_name}:10m rate={p.stats.suggest_rps}r/s;\n"

    config += """
# 3. ä½¿ç”¨ç¤ºä¾‹
server {
    # å…¨å±€é™æµ
    location / {
        limit_req zone=global burst=20 nodelay;
        limit_req_status 429;
    }
"""

    for p in path_analyses:
        if p.is_sensitive:
            zone_name = re.sub(r'[^a-zA-Z0-9]', '_', p.path.lstrip('/')) or "sensitive"
            burst = max(5, p.stats.suggest_rps // 2)
            config += f"""
    # æ•æ„Ÿè·¯å¾„: {p.path}
    location {p.path} {{
        limit_req zone={zone_name} burst={burst} nodelay;
        limit_req_status 429;
        # å»ºè®®: {p.stats.human_readable}
    }}
"""

    config += "}\n"
    return config


def generate_status_stats(status_code_stats: Counter) -> List[Dict[str, Any]]:
    """ç”ŸæˆçŠ¶æ€ç ç»Ÿè®¡"""
    total = sum(status_code_stats.values())
    if total == 0:
        return []

    def get_type(code: str) -> str:
        try:
            c = int(code)
            if 200 <= c < 300:
                return "success"
            elif 300 <= c < 400:
                return "redirect"
            elif 400 <= c < 500:
                return "client_error"
            elif 500 <= c < 600:
                return "server_error"
            else:
                return "other"
        except ValueError:
            return "unknown"

    return [
        {
            'code': code,
            'count': count,
            'percentage': round(count / total * 100, 2),
            'type': get_type(code)
        }
        for code, count in status_code_stats.most_common()
    ]


# ==================== ä¸»ç¨‹åº ====================

def print_cli_summary(
        start: datetime,
        end: datetime,
        total: int,
        global_result: Dict[str, Any],
        path_analyses: List[PathAnalysis],
        ip_analyses: List[IPAnalysis],
        risk_items: List[RiskItem],
        target_ip: Optional[str] = None,
        target_path: Optional[str] = None
):
    """æ‰“å°CLIæ‘˜è¦"""
    print("\n" + "=" * 70)
    print("ğŸ“ˆ Nginx è¯·æ±‚é¢‘ç‡åˆ†ææ‘˜è¦")
    if target_ip or target_path:
        filters = []
        if target_ip:
            filters.append(f"IP={target_ip}")
        if target_path:
            filters.append(f"è·¯å¾„={target_path}")
        print(f"ç­›é€‰æ¡ä»¶: {', '.join(filters)}")
    print("=" * 70)
    print(f"æ—¶é—´çª—å£: {start.strftime('%Y-%m-%d %H:%M')} â†’ {end.strftime('%Y-%m-%d %H:%M')}")
    print(f"æ€»è¯·æ±‚æ•°: {total:,}\n")

    base = global_result['base']
    print("ğŸŒ å…¨å±€ QPS:")
    print(f"  å¹³å‡: {base.avg_qps:.1f} | å³°å€¼: {base.max_qps:.0f} | P95: {base.p95_qps:.1f} | P99: {base.p99_qps:.1f}")
    print(f"ğŸ’¡ å»ºè®®å…¨å±€é™æµ: {base.suggest_rps} r/s ({base.human_readable})\n")

    print("â±ï¸  å¤šæ—¶é—´çª—å£é™æµå»ºè®®:")
    for name in ['10m', '30m', '1h', '24h']:
        w = global_result['windows'][name]
        print(f"  {name:>4} â†’ {w.suggest_rps:>3} r/s ({w.human_readable})")

    sensitive = [p for p in path_analyses if p.is_sensitive]
    if sensitive:
        print(f"\nğŸš€ æ•æ„Ÿè·¯å¾„é™æµå»ºè®® (å…±{len(sensitive)}ä¸ª):")
        for p in sensitive[:10]:
            print(f"  {p.path:<35} â†’ {p.stats.suggest_rps:>2} r/s ({p.stats.human_readable})")

    print(f"\nğŸ–¥ï¸  Top 10 IP (å…±{len(ip_analyses)}ä¸ª):")
    for ip in ip_analyses[:10]:
        print(
            f"  {ip.ip:<17} â†’ è¯·æ±‚:{ip.stats.total_requests:>7,} | P95:{ip.stats.p95_qps:>5.1f} | è·¯å¾„:{ip.unique_paths:>3}")

    if risk_items:
        print(f"\nâš ï¸  é«˜é£é™©è¡Œä¸º (Top 5 / å…±{len(risk_items)}ä¸ª):")
        for r in risk_items[:5]:
            print(f"  {r.ip} â†’ {r.path}")
            print(f"    è¯„åˆ†:{r.risk_score:.1f} | {r.reason}")

    print("\n" + "=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="Nginx è¯·æ±‚é¢‘ç‡è‡ªåŠ¨åŒ–åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨ (ä¼˜åŒ–ç‰ˆ v3)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s access.log --last 24h
  %(prog)s *.log.gz --target-ip 192.168.1.100
  %(prog)s access.log --sensitive-paths "/api,/admin" --output report.html
        """
    )

    parser.add_argument("logfiles", nargs='+', help="ä¸€ä¸ªæˆ–å¤šä¸ªæ—¥å¿—æ–‡ä»¶ï¼ˆæ”¯æŒ .gzï¼‰")
    parser.add_argument("--last", help="åˆ†ææœ€è¿‘æ—¶é—´ï¼Œå¦‚ '1h', '24h', '7d'")
    parser.add_argument("--sensitive-paths", help="è‡ªå®šä¹‰æ•æ„Ÿè·¯å¾„ï¼ˆé€—å·åˆ†éš”ï¼‰")
    parser.add_argument("--target-ip", help="æŒ‡å®šåˆ†æçš„IPåœ°å€")
    parser.add_argument("--target-path", help="æŒ‡å®šåˆ†æçš„è·¯å¾„")
    parser.add_argument("--output", help="HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    parser.add_argument("--workers", type=int, default=4, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰")

    args = parser.parse_args()

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    if not args.output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output = f"rate_report_{timestamp}.html"

    # æ•æ„Ÿè·¯å¾„
    sensitive_paths = set(DEFAULT_SENSITIVE_PATHS)
    if args.sensitive_paths:
        sensitive_paths.update(
            p.strip().rstrip('/') for p in args.sensitive_paths.split(',') if p.strip()
        )

    # æ—¶é—´çª—å£
    time_window = None
    if args.last:
        try:
            if args.last.endswith('h'):
                delta = timedelta(hours=int(args.last[:-1]))
            elif args.last.endswith('d'):
                delta = timedelta(days=int(args.last[:-1]))
            else:
                raise ValueError("--last å¿…é¡»ä»¥ h æˆ– d ç»“å°¾ï¼Œå¦‚ '1h', '24h', '7d'")
            now = datetime.now().astimezone()
            time_window = (now - delta, now)
        except Exception as e:
            print(f"[ERROR] æ—¶é—´çª—å£è§£æå¤±è´¥: {e}", file=sys.stderr)
            sys.exit(1)

    # åˆ†ææ—¥å¿—
    try:
        data = analyze_logs_optimized(
            args.logfiles,
            time_window,
            sensitive_paths,
            args.target_ip,
            args.target_path,
            args.workers
        )
    except Exception as e:
        print(f"[ERROR] æ—¥å¿—åˆ†æå¤±è´¥: {e}", file=sys.stderr)
        traceback.print_exc()
        sys.exit(1)

    if data['total_processed'] == 0:
        print("âŒ æŒ‡å®šæ¡ä»¶å†…æ— æœ‰æ•ˆæ—¥å¿—", file=sys.stderr)
        sys.exit(1)

    # ç»Ÿè®¡åˆ†æ
    cache = TimeWindowCache()
    global_result = analyze_global_stats(data['global_per_sec'], cache)
    path_analyses = analyze_paths(data['path_per_sec'], sensitive_paths, data['path_ips'], cache)
    ip_analyses = analyze_ips(data['ip_per_sec'], data['ip_paths'], data['ip_errors'])
    risk_items = analyze_risks(data['ip_path_per_sec'], data['ip_errors'])
    status_stats = generate_status_stats(data['status_code_stats'])

    # ç”Ÿæˆé…ç½®
    nginx_conf = generate_nginx_config(global_result, path_analyses)

    # ç”ŸæˆHTML
    start, end = data['time_window']
    html_content = generate_html_report(
        start, end, data['total_processed'],
        global_result, path_analyses, ip_analyses, risk_items, status_stats,
        nginx_conf, args.target_ip, args.target_path
    )

    # ä¿å­˜HTML
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(html_content)

    # ä¿å­˜JSON
    json_path = Path(args.output).with_suffix('.json')
    json_data = {
        'summary': {
            'time_window': [start.isoformat(), end.isoformat()],
            'total_requests': data['total_processed'],
            'global_stats': asdict(global_result['base']),
            'path_count': len(path_analyses),
            'ip_count': len(ip_analyses),
            'risk_count': len(risk_items)
        },
        'paths': [
            {
                'path': p.path,
                'stats': asdict(p.stats),
                'is_sensitive': p.is_sensitive,
                'unique_ips': p.unique_ips
            }
            for p in path_analyses[:100]
        ],
        'ips': [
            {
                'ip': ip.ip,
                'stats': asdict(ip.stats),
                'unique_paths': ip.unique_paths,
                'error_rate': ip.error_rate
            }
            for ip in ip_analyses[:100]
        ],
        'risks': [
            {
                'ip': r.ip,
                'path': r.path,
                'risk_score': r.risk_score,
                'reason': r.reason
            }
            for r in risk_items[:50]
        ]
    }

    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)

    # æ‰“å°æ‘˜è¦
    print_cli_summary(start, end, data['total_processed'], global_result,
                      path_analyses, ip_analyses, risk_items,
                      args.target_ip, args.target_path)

    print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {os.path.abspath(args.output)}")
    print(f"ğŸ’¾ JSON æ•°æ®å·²ä¿å­˜è‡³: {os.path.abspath(json_path)}")
    print("âœ… åˆ†æå®Œæˆ!\n")


if __name__ == '__main__':
    main()
